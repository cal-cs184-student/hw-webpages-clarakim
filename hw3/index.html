<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Name: Clara Kim</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-clarakim/">https://cal-cs184-student.github.io/hw-webpages-clarakim/</a>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-clarakim-3">https://github.com/cal-cs184-student/sp25-hw3-clarakim-3</a>
		
		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		In this project, I started off with generating rays in the world space, and calculating intersections of rays with triangles and spheres.
		I implemented BVH with a midpoint heuristic to speed up the rending process with partitioning based on bounding boxes. I set up zero bounce, and then 
		implemeted direct lighting with uniform hemisphere sampling and light sampling (one bounce), which the light sampling rendered with much less noise.
		Lastly, I implemented indirect lighting with multiple bounces after the first one bounce, terminating with russian roulette and finally using adaptive sampling
		to add the final touches for rendering the images.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		In camera.cpp, the cameral model was simulated as a pinhole, generating rays that pass through the image plane into the scene.
		Rays were generated by calculating sensorX, sensorY = (2 * x - 1) * tan(radians(hFov) / 2), (2 * y - 1) * tan(radians(vFov) / 2), which first
		mapped the pixel (x,y) coordinates into camera's view. The ray, originating from (0,0,0) points towards the sensor coordinates in camera space,
		and the ray was then transformed to world space. Lastly, the valid range of the ray was set with nClip and fClip. Before the primitive intersection parts were
		implemented, pixel samples were generated to estimate the integral of radiance over each pixel, which would be used to calculate shading down the rasterization pipeline.
		<br><br>
		After ray genration, there needed to be checks for intersections with objects in the scene, so Ray-Triangle Intersection was implemented.
		MÃ¶ller-Trumbore algorithm was used, which is an optimization technique to compute the intersection between a ray and a triangle.
		First, the edge vectors e1 and e2 were calculated, then s, s1, and s2 were calculated with cross products (except s). The rest of the components in the formula were
		all calculated using the equations from lecture to see if an intersection existed. Once it was found that the triangle has an intersection, the barycentric coordinates were calculated
		to see if t was within ray's min_t and max_t, and then a >= 0, b >= 0, and a + b <= 1 were also checked to see if the intersection was inside the triangle.
		<br>
		The Ray-Sphere intersection was also similar, using the formulas from lecture slides to check if there was an intersection for the ray and if the intersection was inside the sphere.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part1/CBspheres.png" width="300px"/>
				  <figcaption>CBSpheres.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part1/banana.png" width="300px"/>
				  <figcaption>banana.dae</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="part1/rabbit_1.png" width="300px"/>
					<figcaption>CBbunny.dae</figcaption>
				  </td>
			  </tr>
			</table>
		</div>

		<h2>Part 2: Bounding Volume Hierarchy</h2>
		For BVH, I first have a big bounding box that encloses all the primitives, and unless the number of primitives is 
		small enough (less than the max_leaf_size), which then we would make that a leaf, we need to split the bounding box into 
		left and right partitions. To ensure we split efficiently and effectively, we split along the largest axis (between xyz), which is 
		calculated by using extent. Once the largest axis is determined, I chose the heuristic of splitting at the midpoint of the space at the selected axis, and partitioned
		the primitives to left and right based on their center (centroid) in respect to the split_point previously determined. The midpoint gets adjusted
		based on the left and right primitives, because they would have a new start (or end) range once the initial bounding box is split in half. 
		In the case that all primitives belong on one side of the split point, I chose to just take the midpoint of the primitives.
		Lastly, I recurse on the left and right sides of the node, adjusting the start and end like I previously mentioned.<br>
		This is the BVH algorithm, and the rays are checked in each of the left and right boxes recursively to see if it intersects (hits), and the closest point where it intersects
		will be returned.<br>
		I chose this heuristic because splitting by the largest axis (dimension) would most effectively split/shrink the large bounding boxes, and it is extremely easy, quick, and efficient to 
		just split at the middle (spatially) and partition the primitives left and right with centeroid comparison.
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="part2/man.png" width="300px"/>
					<figcaption>Maxplanck.dae</figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="part2/lucy.png" width="300px"/>
					<figcaption>CBlucy.dae</figcaption>
				  </td>
				  <td style="text-align: center;">
					  <img src="part2/walle.png" width="300px"/>
					  <figcaption>wall-e.dae</figcaption>
					</td>
			  </tr>
			</table>
		</div>

		<br><br>
		When I rendered CBbunny.dae before the BVH implementation, it took extremely long to render, taking 779.5 seconds. After BVH, it only took 0.0131 seconds.
		I also tried rendering peter.dae, which took 641.3 seconds, and then 0.0115 seconds afterwards.
		Since we are only checking bounding boxes where a ray intersect occurs instead of checking everything, it speeds up the rendering process extremely (almost uncomparably), saving a lot of time.

		<h2>Part 3: Direct Illumination</h2>
		For Uniform Hemisphere Sampling, we want to know how much light arrived at an intersection point from somewhere,
		so the rendering equation and Monte Carlo estimater is used to integrate over all the light arriving in a hemisphere around the hit_p. 
		I first take a uniform random sample (direction) from the hemisphere, which I then converted the sampled direction to the world space (o2w* w_in_local) and used this to trace the "shadow_ray" towards, in order to check 
		for intersection with objects. If the shadow ray hits an emissive surface (light), I calculated the BDRF, then weighed it using cos_theta / pdf. This light contribution
		is continuously accumulated to L_out, which is returned at the end after being divided by num_samples in order to take the average.
		<br><br>
		For Importantance Sampling Lights, instead of sampling from all directions in the hemisphere, I loop over all the scene lights, and then use sample_L to generate a direction in the world, 
		distance to light, and pdf. If the light is behind the surface, then it is not used. Same as previous, the shadow ray's min_t is set to EPS_F but max_t is set to distToLight offset by EPS_F because that's the new max since we want
		to track from the light sources. This time, if the light does NOT hit an object, then it is added to L_out as light-contribution, because we know that this sampled light did not meet any obstructions.
		Again, the BRDF is calcualted, weighted by cosine factor and PDF, then the light contribution is accumulated then averaged by division by num_samples.
		<br><br>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="part3/CBbunny_H_64_32.png" width="400px"/>
				<figcaption>CBbunny.dae (Uniform Hemisphere)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part3/CBbunny_H_64_32_sample.png" width="400px"/>
				<figcaption>CBbunny.dae (Light Sampling)</figcaption>
			  </td>
			</tr>
			<tr>
			  <td style="text-align: center;">
				<img src="part3/H_sphere_64_32.png" width="400px"/>
				<figcaption>CBSphere_Lamberatian.dae (Uniform Hemisphere)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part3/sphere_64_32.png" width="400px"/>
				<figcaption>CBSphere_Lamberatian.dae (Light Hemisphere)</figcaption>
			  </td>
			</tr>
		</table>
		<br><br>
		Uniform Hemisphere sampling produces more noisy image because it samples directions randomly, and often misses direct light sources, which leads to high variane.
		Lighting sampling directly samples from all the light sources, which reduces variance and improves efficiency, which results in more cleaner image.
		The edges of the images are much sharper and cleaner, with better shadow definition and realistic illumination, which are all quite noticeable in these comparisons.
		The shadings for the lighting sampling are also much smoother, and walls actually look like a clean, flat surface in comparison to the noisy uniform hemisphere sampling images.
		<h3>Noise Level Comparison in Soft Shadows</h3>
		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="part3/dragon_l1.png" width="400px"/>
				<figcaption>dragon.dae (1 light ray)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part3/dragon_l4.png" width="400px"/>
				<figcaption>dragon.dae (4 light rays)</figcaption>
			  </td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="part3/dragon_l16.png" width="400px"/>
					<figcaption>dragon.dae (16 light rays)</figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="part3/dragon_l64.png" width="400px"/>
					<figcaption>dragon.dae (64 light rays)</figcaption>
				  </td>
			</tr>
		</table>

		<h2>Part 4: Global Illumination</h2>
		The indirect lighting is implemented by first computing the one_bounce (direct), and then accumulating light contributions with each bounce and recursion.
		sample_f() picks random direction for next bounce according to the material's BSDF, and this bounce direction is transformed into world space, and since we took a bounce,
		create a new recursive ray for the next bounce starting from hit_p with a decremented depth. I checked for intersection for this next ray, and continue to accumulate and recurse onto the 
		L_indirect. I either stop once I reach the max bounce depth, or with Russian Roulette implementation, terminate with a probability of 0.3 (or 0.05 for the purpose of the rendering image) which greatly improves performance.
		<br><br>
		** did not have time to render the rest... but checked that sanity tests and renderings were successful :(
			<br><br>
		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="part4/m0af.png" width="200px"/>
				<figcaption>CBbunny.dae (0 bounce, false)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part4/m0at.png" width="200px"/>
				<figcaption>CBbunny.dae (0 bounce, true)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part4/m1at.png" width="200px"/>
				<figcaption>CBbunny.dae (1 bounce, true)</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part4/m5at.png" width="200px"/>
				<figcaption>CBbunny.dae (5 bounce, true)</figcaption>
			  </td>
			</tr>
			
		</table>

		<h2>Part 5: Adaptive Sampling</h2>
		Adaptive Sampling is used to optimize sample allocation across pixels, so that instead of a fixed number of samples per pixel,
		the number of samples is adjusted based on the variance in pixel color. High-variance regions such as edges and shadows are allocated
		more pixels in order to reduce noise, while low-variance regions such as constant surface get fewer samples. This technique is effective in improving
		efficiency while maintaining the image quality.
		<br><br>
		Adaptive sampling was implemented in raytrace_pixel, first using radiance.illum() to calculate s1 (sum of xk) and s2 (sum of xk squared) which was then used 
		to calculate statistics (mean, std, variance) for the formula for pixel convergence. The sample count was tracked alongside the incrementing i count for the sample,
		which was used to check whether pixel has converged every samplesPerBatch samples (=32) instead of costly checking for convergence every time. Once "I" was calculated
		and it was determined that the pixel has converged, the average radiance was calculated by dividing by the sample count (instead of reaching the full ns_aa samples every time),
		achieving the purpose of adaptive sampling.
		<br><br>
		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
			  <td style="text-align: center;">
				<img src="part5/bunny_final.png" width="300px"/>
				<figcaption>CBbunny.dae</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part5/dragon_final.png" width="300px"/>
				<figcaption>dragon.dae</figcaption>
			  </td>
			  <td style="text-align: center;">
				<img src="part5/sphere_5_final.png" width="300px"/>
				<figcaption>CBSphere_Lamberatian.dae (Rate)</figcaption>
			  </td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="part5/bunny_final_rate.png" width="300px"/>
				  <figcaption>CBbunny.dae (Rate)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part5/dragon_final_rate.png" width="300px"/>
				  <figcaption>dragon.dae (Rate)</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="part5/sphere_5_final_rate.png" width="300px"/>
					<figcaption>CBSphere_Lamberatian.dae (Rate)</figcaption>
				  </td>
			  </tr>
		</table>

		</div>
	</body>
</html>